{
  "hash": "9b0e18d8a00d812986d56fd801a9fd09",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Cervical Cancer Risk Prediction\"\nsubtitle: \"INFO 523 - Final Project\"\nauthor: \n  - name: \"Team Okhawere\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"Classification Problem - Predicting Cervical Cancer\"\nformat:\n   html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  echo: false\njupyter: python3\n---\n\n## Abstract\n\nEarly identification of patients at risk for biopsy-confirmed cervical pathology can support targeted follow-up in resource-limited settings. Using the UCI cervical risk-factors dataset, we sought to build a supervised classifier to predict biopsy outcome from demographic, behavioral, and STD-history. We compared Logistic Regression, Random Forest, and XGBoost with 5-fold stratified cross-validation using Average Precision (PR-AUC). On the held-out test set (n=167), the LR model achieved Average Precision 0.145 and ROC-AUC 0.629. While this performance demonstrates the potential of machine learning in risk assessment, it also underscores the need for a balanced approach to optimize model utility in a clinical context.\n\n## Introduction\n\nCervical cancer remains a significant health concern, particularly in settings where access to standard screening and vaccination is limited. Current diagnostic pathways, which often involve extensive follow-up procedures like colposcopy and biopsy, can strain clinical resources, leading to delays for high-risk patients. To address this, a non-invasive tool that uses information already gathered during patient intake, such as demographics, sexual history, and smoking habits, could help clinicians prioritize follow-up evaluations. Given the above, using the UCI cervical risk-factors dataset, this study seeks to evaluate the potential of such a lightweight prediction model to identify patients at high risk of a positive biopsy without adding extra clinical burden, and factors contributing to most of the risk. We aim to develop a tool that functions as a triage aid, ensuring limited resources are directed toward those who are most likely to benefit from further diagnostic steps.\n\n\n\n\n\n## Data Preprocessing\n\n\n\nThe population for this study consists of 835 female patients from the Hospital Universitario de Caracas in Caracas, Venezuela. The total number variables in the dataset is 36.\n\n### Date cleaning\n\n**Dropped Columns:** Columns containing diagnostic information that would cause data leakage, high correlation and rare or missing events were removed.\n\n1.  Hinselmann, Schiller, Citology, Dx, Dx:HPV, Dx:CIN, Dx:Cancer\n\n2.  STDs: Time since first diagnosis, STDs (number)\n\n3.  STDs:AIDS, STDs:cervical condylomatosis, STDs:pelvic inflammatory disease, STDs:genital herpes, STDs:molluscum contagiosum, STDs:Hepatitis B, STDs:HPV, STDs:vaginal condylomatosis, STDs:condylomatosis, STDs:vulvo-perineal condylomatosis\n\n**Dropped Duplicates:** A total of np.int64(23) total duplicate rows were identified and removed.\n\n**Correcting Implausible Values:** Records where a patient's age at first sexual intercourse was greater than their current age were corrected. The inconsistent values were replaced with missing values (np.nan).\n\n#### **Missing Data and Feature Engineering**\n\n\n\n1.  Dealing with Systematic Missing Data: Patients with zero recorded STDs, the associated fields (such as \"Time since first diagnosis\" or other specific STD types) were used to impute missing.\n\n2.  Feature Engineering:\n\n-   Years since first intercourse: This new feature is calculated by subtracting the First sexual intercourse age from the patient's Age.\n-   Any history of condylomatosis: presence of any condylomatosis\n\n\n\n### Target Variable\n\nThe target variable, biopsy test (1 indicates cancer detected, 0 indicates no cancer), is the most definitive indicator for cervical cancer. The majority of the patients (approximately 6%) have a positive biopsy result, suggestive of a potential imbalance in the distribution of cancer diagnosis.\n\n::: {#cell-fig-figure-1 .cell message='false' execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Distribution of Biopsy Results in the Cohort](index_files/figure-html/fig-figure-1-output-1.png){#fig-figure-1}\n:::\n:::\n\n\n\n\n\n\n### Relationship of Biopsy and Select Covariates\n\nIn the cohort, patients with a positive biopsy tend to be older, have a longer duration since their first sexual intercourse, and a higher number of sexual partners. Additionally, longer periods of smoking and hormonal contraceptive use are also linked to a higher risk.\n\n::: {#cell-fig-figure-4 .cell message='false' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Relationship between Biopsy and Select Variables](index_files/figure-html/fig-figure-4-output-1.png){#fig-figure-4}\n:::\n:::\n\n\n::: {#tbl-bivariate-results .cell message='false' tbl-cap='Summary of Chi-Square Test' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\n                  Variable  Chi-Square   p-value  Significant\n0                   Smokes    0.390361  0.532110        False\n1  Hormonal Contraceptives    0.008545  0.926350        False\n2                      IUD    1.452592  0.228112        False\n3                     STDs    9.440735  0.002122         True\n4            STDs:syphilis    0.413952  0.519970        False\n5                 STDs:HIV   10.446082  0.001229         True\n6       Any_condylomatosis    5.297113  0.021361         True\n```\n:::\n:::\n\n\nThe presence of any STD, STDs:HIV, any condylomatosis were associated with significant relationship with the biopsy outcome (Table 1). Simialar,the number and timing of STDs were the only numerical variables that showed a statistically significant difference between the two groups. Specifically, STDs: Number of diagnosis, and STDs: Time since first diagnosis all had very low p-values (p\\<0.05), indicating that these factors are significantly different in the positive biopsy group (Table 2).\n\n::: {#tbl-bivariate-results2 .cell message='false' tbl-cap='Summary of Mann-Whitney U Test' execution_count=11}\n\n::: {.cell-output .cell-output-stdout}\n```\n                           Variable  U-statistic        p-value  \\\n0                               Age      18531.5   1.394219e-01   \n1         Number of sexual partners      20050.0   9.949846e-01   \n2          First sexual intercourse      18970.5   2.640520e-01   \n3                Num of pregnancies      14416.0   5.364427e-02   \n4                    Smokes (years)      19392.5   3.421653e-01   \n5               Smokes (packs/year)      19402.0   3.468394e-01   \n6   Hormonal Contraceptives (years)      17469.5   5.674593e-01   \n7                       IUD (years)      15992.0   1.508000e-01   \n8         STDs: Number of diagnosis      18428.0   1.328137e-03   \n9   STDs: Time since last diagnosis      17870.0   1.236808e-03   \n10                           Biopsy          0.0  2.231708e-183   \n11    Years since first intercourse      18527.5   1.713081e-01   \n\n    Median (No Biopsy)  Median (Biopsy)  Significant  \n0                 26.0             28.0        False  \n1                  2.0              2.0        False  \n2                 17.0             17.0        False  \n3                  2.0              3.0        False  \n4                  0.0              0.0        False  \n5                  0.0              0.0        False  \n6                  0.5              0.5        False  \n7                  0.0              0.0        False  \n8                  0.0              0.0         True  \n9                  0.0              0.0         True  \n10                 0.0              1.0         True  \n11                 8.0              9.5        False  \n```\n:::\n:::\n\n\n### Baseline Model Training and Evaluation\n\n## Modeling and Evaluation\n\n#### **Train-Test Split**\n\nThe data was split into stratified training (60%), validation (20%), and testing (20%) sets to ensure an even distribution of positive and negative cases across each partition.\n\n::: {#model-preprocessor .cell message='false' execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFinal Split Sizes:\nTraining set: 501 samples\nValidation set: 167 samples\nTesting set: 167 samples\n```\n:::\n:::\n\n\n#### **Preprocessing Pipeline**\n\nA preprocessing pipeline was defined to handle both numerical and categorical features by using imputation, scaling, and one-hot encoding.\n\n#### **Dealing with Imbalance**\n\nWe used SMOTE oversampling technique to address class imbalance before training.\n\n### Model Selection\n\nWe evaluated there different classification machine learning models:\n\n1.  Logistic Regression (LR): A simple, interpretable model chosen as a baseline for performance comparison.\n\n2.  Random Forest (RF): A powerful ensemble model that reduces overfitting by combining multiple decision trees.\n\n3.  XGBoost (XGB): A highly efficient and scalable implementation of the gradient boosting algorithm. It is often the preferred choice for structured data due to its superior speed and performance.\n\nThe models are compared using 5-fold cross-validation on the training set, with average precision as the primary evaluation metric\n\n::: {#training .cell message='false' execution_count=13}\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--- Baseline Model Comparison (on Training Set) ---\nLogistic Regression: mean average_precision = 0.0855 (±0.0515)\nRandom Forest: mean average_precision = 0.1096 (±0.0611)\nXGBoost: mean average_precision = 0.1199 (±0.0581)\n```\n:::\n:::\n\n\nXGB was the best untuned learner (mean AP 0.119 ± 0.058), with RF close behind (0.109 ± 0.061) while LR is 0.0855 (±0.0515)\n\n### Model Hyperparameter Tuning - Using RandomSearchCV\n\n::: {#hypertuning .cell message='false' execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--- Randomized Search ---\n\nTraining Logistic Regression with 20 iterations...\nFitting 5 folds for each of 20 candidates, totalling 100 fits\nLogistic Regression: best VAL average_precision=0.1169\nBest params: {'classifier__C': np.float64(0.0019145111285710716), 'classifier__class_weight': 'balanced', 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'smote__k_neighbors': 3}\n\nTraining Random Forest with 50 iterations...\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nRandom Forest: best VAL average_precision=0.0921\nBest params: {'classifier__class_weight': None, 'classifier__max_depth': None, 'classifier__max_features': 0.5, 'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 4, 'classifier__n_estimators': 474, 'smote__k_neighbors': 3}\n\nTraining XGBoost with 50 iterations...\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nXGBoost: best VAL average_precision=0.0953\nBest params: {'classifier__colsample_bytree': np.float64(0.8288016796836732), 'classifier__learning_rate': np.float64(0.08013117005374264), 'classifier__max_depth': 2, 'classifier__min_child_weight': 4, 'classifier__n_estimators': 269, 'classifier__reg_alpha': np.float64(0.0256941078055752), 'classifier__reg_lambda': np.float64(0.36234367402567613), 'classifier__subsample': np.float64(0.9533976089065036), 'smote__k_neighbors': 2}\n\n--- Average Precision Results ---\nLogistic Regression 0.1169\nXGBoost            0.0953\nRandom Forest      0.0921\n\nSelected model: Logistic Regression (VAL average_precision=0.1169)\n```\n:::\n:::\n\n\nAfter grid search, LR remianed on the top with average precision of 0.117, followed by RF (0.0953), DCT (0.092).\n\n### Final Model Evaluation\n\n::: {#final-evaluation .cell message='false' execution_count=15}\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--- FINAL TEST METRICS ---\nAverage Precision (PR-AUC): 0.145\nROC-AUC: 0.629\n              precision    recall  f1-score   support\n\n           0      0.948     0.814     0.876       156\n           1      0.121     0.364     0.182        11\n\n    accuracy                          0.784       167\n   macro avg      0.534     0.589     0.529       167\nweighted avg      0.893     0.784     0.830       167\n\nConfusion matrix:\n TN=127 FP=29\n FN=7 TP=4\nSpecificity=0.814 | Sensitivity=0.364 (Recall)\n```\n:::\n:::\n\n\nThe final LR model was evaluated on a test set, where it yielded an AP (PR-AUC) of 0.145 and an ROC-AUC of 0.629. The confusion matrix revealed that while the model has high specificity (0.968), correctly identifying most negative cases, its sensitivity (0.364) is very low, as it missed 4 of the 11 true positive cases. Overall, the tuned LR generalizes modestly (VAL AP 0.117 → TEST AP 0.116); Hence, overfitting is limited.\n\n## Feature Importance\n\n\n\n::: {#cell-fig-figure-5 .cell message='false' execution_count=17}\n\n::: {.cell-output .cell-output-display}\n![Relationship between Biopsy and Select Variables](index_files/figure-html/fig-figure-5-output-1.png){#fig-figure-5}\n:::\n:::\n\n\nTo address our second aim, which is to identify important explanatory variables likely driving the prediction. The Mean absolute SHAP indicated that predictions were dominated by sexual-exposure variables—number of sexual partners, age at first intercourse, and years since first intercourse—followed by number of pregnancies and hormonal contraceptive exposure (both years of use and ever/never status). Age contributed an additional signal, while STD burden/timing and smoking/IUD features provided smaller effects.\n\n-   Hormonal Contraceptives (years) was the strongest driver. High values tend to increase predicted risk.\n-   STDs: Number of diagnoses — more diagnoses push predictions up.\n-   Years since first intercourse — longer history generally increases risk.\n-   Age: older age trends up (modest effect).\n-   STD :Time since last diagnosis — more recent STD tend to increase risk.\n-   Number of pregnancies increase the risk\n\n## Conclusion\n\nThis project demonstrates that a regularized Logistic Regression model, trained with imbalance-aware preprocessing strategy, can offer the best performance among the candidates for predicting a positive cervical cancer biopsy.n baseline cross-validation on the training set, mean average precision (AP) was 0.0855 for Logistic Regression, 0.1096 for Random Forest, and 0.1199 for XGBoost. After randomized hyperparameter search on the combined train+validation data, Logistic Regression achieved the highest validation AP of 0.1169, and was selected as the final model.\n\nOur model achieved a modest ROC-AUC of 0.629 and PR-AUC of 0.145 in the test set, and at a threshold of 0.5 indicating a modest ability to discriminate between positive and negative cases. Its high specificity (0.814) and low sensitivity (0.364) with TN=127, FP=29, FN=7, TP=4, and precision = 0.121reveal a critical trade-off. This means the model captures about a third of true positives but produces many false positives. Still, it misses a significant portion of the actual positive cases (resulting in many false negatives). At this threshold, it functions better as a triage aid than as a replacement for diagnostic testing.\n\nFor practical application, the model's utility could be enhanced by focusing on improving its ability to detect the rare positive cases. Future work should explore strategies such as: 1. decision-threshold calibration guided by precision–recall trade-offs 2. Incorporating richer clinical variables (e.g., specific HPV strain information) to improve the model's predictive power. Ultimately, this work highlights the potential for machine learning to assist in clinical decision-making by identifying key risk factors and providing a more informed basis for targeted follow-up.\n\n### Limitation\n\nThe study has several limitations. First, the model was built and evaluated on a single-center dataset which limits it generazilibiy. Secondly, the model relies exclusively on demographic, behavioral, and self-reported STD history features that may reflect local practices and does not incorporate richer clinical variables, such as specific HPV viral types or Pap smear results, which are highly predictive in real-world diagnostics. Thirdly, despite using SMOTE to address the significant class imbalance, the final model still struggles to reliably identify the minority class, as evidenced by its low recall. Finally, the low sensitivity of the model is a critical flaw, and its moderate predictive power is only slightly better than random chance to effectively discriminate between patients with and without cervical pathology.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}